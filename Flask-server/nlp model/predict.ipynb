{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PySastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3c1b7VFCI4d",
        "outputId": "7c4412f3-1069-4ac0-e477-eef9d6ae5d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PySastrawi\n",
            "  Downloading PySastrawi-1.2.0-py2.py3-none-any.whl.metadata (892 bytes)\n",
            "Downloading PySastrawi-1.2.0-py2.py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.6/210.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PySastrawi\n",
            "Successfully installed PySastrawi-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import string"
      ],
      "metadata": {
        "id": "m8gBdpSODHIa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load saved models, vectorizer, and selector"
      ],
      "metadata": {
        "id": "SmX3xtDzcc0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer_factory = StemmerFactory()\n",
        "stemmer = stemmer_factory.create_stemmer()\n",
        "\n",
        "stop_factory = StopWordRemoverFactory()\n",
        "stop_words = stop_factory.get_stop_words()"
      ],
      "metadata": {
        "id": "iY4eeFqeCQKz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles = ['devops', 'backend', 'frontend', 'android', 'ios']"
      ],
      "metadata": {
        "id": "NG9Xa-P7D8Lj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/model/bernoulli_model_v1_c5_e201.pickle\", \"rb\")\n",
        "bernoulli = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "file = open(\"/content/model/mnb_model_v1_c5_e201.pickle\", \"rb\")\n",
        "multinomial = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "file = open(\"/content/model/logistic_regression_model_v1_c5_e201.pickle\", \"rb\")\n",
        "logistic_regression = pickle.load(file)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "hwC-FioeC0JK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/tfidf_vectorizer.pkl', 'rb') as f:\n",
        "    tfidf = pickle.load(f)\n",
        "\n",
        "with open('/content/select_kbest.pkl', 'rb') as f:\n",
        "    selector = pickle.load(f)"
      ],
      "metadata": {
        "id": "xkUvmUOHcG2s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalization and text preprocessing"
      ],
      "metadata": {
        "id": "pDPEUelNcnl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VoteClassifier():\n",
        "    def __init__(self, *classifiers):\n",
        "        self._classifiers = classifiers\n",
        "\n",
        "    def classify(self, prep_data):\n",
        "        votes = [0] * len(titles)\n",
        "        for c in self._classifiers:\n",
        "            v = c.predict_proba(prep_data)\n",
        "            votes += v\n",
        "        votes /= len(self._classifiers)\n",
        "        return votes\n"
      ],
      "metadata": {
        "id": "7qZcy6CQDcBC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def normalize_negation(text):\n",
        "    words = word_tokenize(text)\n",
        "    negation = ['tidak', \"bukan\"]\n",
        "    document = []\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        if any(string in words[i] for string in negation):\n",
        "            antonym_found = False\n",
        "            synset = wordnet.synsets(words[i+1])\n",
        "            for s in synset:\n",
        "                for synonym in s.lemmas():\n",
        "                    for antonym in synonym.antonyms():\n",
        "                        document.append(antonym.name())\n",
        "                        antonym_found = True\n",
        "                        break\n",
        "                    if antonym_found == True:\n",
        "                        break\n",
        "                if antonym_found == True:\n",
        "                    break\n",
        "            i+=1\n",
        "        else:\n",
        "            document.append(words[i])\n",
        "        i+=1\n",
        "    return document"
      ],
      "metadata": {
        "id": "k5LEuYxCDllh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"saya senang dengan pemrograman yang berkaitan dengan server dan API\""
      ],
      "metadata": {
        "id": "L04n5kkqDtkT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [s]\n",
        "prep_data = []\n",
        "# data = normalize_negation(data)\n",
        "\n",
        "for d in data:\n",
        "    d = [stemmer.stem(dt).lower() for dt in d.split(' ')]\n",
        "    d = [dt for dt in d if dt not in stop_words]\n",
        "    prep_data.append(' '.join(d))"
      ],
      "metadata": {
        "id": "SAgBqNFuDwN_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_data_tfidf = tfidf.transform(prep_data)\n",
        "\n",
        "prep_data_selected = selector.transform(prep_data_tfidf)\n",
        "\n",
        "prep_data_2d = prep_data_selected"
      ],
      "metadata": {
        "id": "dqTuTff6dMPt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = VoteClassifier(bernoulli, multinomial, logistic_regression)"
      ],
      "metadata": {
        "id": "Hp8DGp1TdYwF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = classifier.classify(prep_data_2d)\n",
        "print(res)\n",
        "print(titles[np.argmax(res)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcx0zs2MdZx9",
        "outputId": "2f877d9e-57f3-4dc0-8b76-3c1484a85ff9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.05613507 0.3167541  0.16029738 0.16440968 0.30240377]]\n",
            "backend\n"
          ]
        }
      ]
    }
  ]
}